{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# based on UO-Historical-Noise-Download.ipynb\n",
    "import requests, os, io\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "api_date_string_format = \"%Y%m%d%H%M%S\"\n",
    "nsets = [\n",
    "    [1409, 1410],  # taxi rank at the station\n",
    "    [1408, 1414],  # lights near the station\n",
    "    [2604, 2606, 2603, 2602, 2605, ],  # road between uni and USB\n",
    "    [2753, 2752, ],  # nr gosforth high street\n",
    "    [2902, 2904, 2903, ],  # regent centre metro + bus stop (3/6)\n",
    "    [1207, 1206, 1202, ],  # bensham, Symphony Court\n",
    "    [1006, 1007, 1002, ],  # bensham, St George's Church\n",
    "    [1701, 1702, ],  # Centre, Westgate-St James Intersection\n",
    "]\n",
    "sensorList = []\n",
    "for subgroup in nsets:\n",
    "    for sensor_id in subgroup:\n",
    "        sensorList.append('PER_EMOTE_{id}'.format(id=sensor_id))\n",
    "\n",
    "base_url = r'https://newcastle.urbanobservatory.ac.uk/api/v1.1/sensors/'\n",
    "\n",
    "\n",
    "raw_data_cache_path = os.path.join('../cache/','noise', 'raw',)\n",
    "resampled_path = os.path.join('../cache/','noise', 'processed')\n",
    "if not os.path.exists(raw_data_cache_path):\n",
    "    os.makedirs(raw_data_cache_path)\n",
    "if not os.path.exists(resampled_path):\n",
    "    os.makedirs(resampled_path)\n",
    "# Call packaged into monthly chunks\n",
    "# This takes significant time\n",
    "print(datetime.now().strftime('%H:%M:%S'))\n",
    "\n",
    "for sensor_name in sensorList:\n",
    "    print(sensor_name)\n",
    "    min_date = datetime(2020,1,1)\n",
    "\n",
    "    combBaseline = pd.DataFrame()\n",
    "    pandas_pickle_path = os.path.join(raw_data_cache_path, 'noise-data-' + sensor_name + '.pkl')\n",
    "    resample_pandas_pickle_path = os.path.join(resampled_path, 'noise-data-' + sensor_name + '.pkl')\n",
    "    if os.path.exists(pandas_pickle_path):\n",
    "        try:\n",
    "            combBaseline = pd.read_pickle(pandas_pickle_path)\n",
    "\n",
    "            min_date = pd.to_datetime(combBaseline['Timestamp']).max()\n",
    "\n",
    "            min_date = datetime(min_date.year,min_date.month,1)\n",
    "\n",
    "            combBaseline = combBaseline[pd.to_datetime(combBaseline['Timestamp']) < min_date]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    months = pd.date_range(min_date, datetime.now(),\n",
    "                           freq='MS').strftime(\"%Y/%m\").tolist()\n",
    "\n",
    "    for m in months:\n",
    "        print(m)\n",
    "        dataCall = base_url + sensor_name + '/data/cached/Sound/{m}/csv/'.format(m=m)\n",
    "\n",
    "\n",
    "        r = requests.get(dataCall)\n",
    "\n",
    "        # Check if API call succesfuly, merge into one dataframe\n",
    "        if r.status_code != 404:\n",
    "            sensorData = pd.read_csv(io.StringIO(r.text))\n",
    "            combBaseline = combBaseline.append(sensorData, ignore_index=True)\n",
    "\n",
    "\n",
    "    min_date = pd.to_datetime(combBaseline['Timestamp']).max()\n",
    "\n",
    "    data_params = dict(\n",
    "        data_variable='Sound',\n",
    "        starttime=min_date.strftime(api_date_string_format),\n",
    "        endtime=datetime.now().strftime(api_date_string_format)\n",
    "    )\n",
    "    print(data_params)\n",
    "\n",
    "    r = requests.get(base_url + sensor_name + '/data/csv/', data_params)\n",
    "    print(r.url)\n",
    "\n",
    "    sensorData = pd.read_csv(io.StringIO(r.text))\n",
    "    print(sensorData['Timestamp'])\n",
    "\n",
    "    combBaseline = combBaseline.append(sensorData, ignore_index=True)\n",
    "    combBaseline = combBaseline[pd.to_datetime(combBaseline['Timestamp']) > datetime(2020,1,1)]\n",
    "    pd.to_pickle(combBaseline, pandas_pickle_path)\n",
    "    combBaseline['Value'] = combBaseline['Value'].to_numpy()\n",
    "\n",
    "    combBaseline.index = pd.to_datetime(combBaseline['Timestamp'])\n",
    "\n",
    "    combBaseline.sort_index()\n",
    "\n",
    "    print(combBaseline.index[1],combBaseline.index[0])\n",
    "    gaps = combBaseline.index[1:]-combBaseline.index[:-1]\n",
    "    td = timedelta(seconds=int(pd.DataFrame(gaps.total_seconds().to_list()).mode()[0][0]))\n",
    "\n",
    "\n",
    "    daily_sample = combBaseline.resample('24H')\n",
    "\n",
    "    expected_readings = (timedelta(hours=24).total_seconds()/td.total_seconds())\n",
    "    print()\n",
    "    median = daily_sample.median()\n",
    "    median['data_prop'] = daily_sample.count()['Variable']/expected_readings\n",
    "    print(median)\n",
    "\n",
    "    pd.to_pickle(median,resample_pandas_pickle_path)\n",
    "# Save as a Pickle file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}