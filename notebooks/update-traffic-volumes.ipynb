{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import urllib.request\n",
    "import dateutil.parser\n",
    "import dateutil.rrule\n",
    "import dateutil.tz\n",
    "import datetime\n",
    "import re\n",
    "import gc\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tzUTC = dateutil.tz.gettz('UTC')\n",
    "tzLocal = dateutil.tz.gettz('Europe/London')\n",
    "\n",
    "# Used across all of the plots\n",
    "dateToday = datetime.datetime.combine(datetime.date.today(), datetime.datetime.min.time()).replace(tzinfo=tzUTC)\n",
    "baselineEnd = datetime.datetime.strptime('2020-03-16T23:59:59Z', '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=tzUTC)\n",
    "\n",
    "resampleFrequency = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't do anything with old data yet, because of the large number of gaps in recent...\n",
    "try:\n",
    "    dfPointInterpTsOld = pd.read_pickle('../cache/recent-traffic-volumes-pd.pkl')\n",
    "    dfPointInterpTsOld = dfPointInterpTsOld[dfPointInterpTsOld.index < dateToday - pd.Timedelta(days=2)]\n",
    "    previousDataEnd = np.max(dfPointInterpTsOld.index).replace(tzinfo=tzLocal).astimezone(tzUTC)\n",
    "    print('Loaded previous data.')\n",
    "    print('  %s' % previousDataEnd)\n",
    "except:\n",
    "    dfPointInterpTsOld = None\n",
    "    previousDataEnd = baselineEnd\n",
    "    print('No existing data could be loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all of the journey time pair links\n",
    "\n",
    "print('Last updated %s' % (datetime.datetime.now(tzLocal).strftime('%d %B %Y %H:%M')))\n",
    "\n",
    "anprRequestBase = 'https://api.newcastle.urbanobservatory.ac.uk/api/v2/sensors/entity'\n",
    "\n",
    "# Fetch a list of all the car parks...\n",
    "anprLinks = {}\n",
    "anprRequestPage = 1\n",
    "anprResponse = None\n",
    "\n",
    "anprNameMatcher = re.compile('^(.*) - (.*) to (.*)$')\n",
    "\n",
    "while anprResponse is None or len(anprResponse) > 1:\n",
    "    anprResponse = json.loads(\n",
    "        urllib.request.urlopen(\n",
    "            '%s?name=\"Vehicle%%20monitoring%%20pair%%20\"&page=%u' % (anprRequestBase, anprRequestPage)\n",
    "        ).read().decode('utf-8')\n",
    "    )['items']\n",
    "\n",
    "    anprRequestPage = anprRequestPage + 1\n",
    "\n",
    "    for journeyTimeLink in anprResponse:     \n",
    "        for feed in journeyTimeLink['feed']:\n",
    "            systemCodeNumber = feed['brokerage'][0]['sourceId'].split(':')[0]\n",
    "            \n",
    "            # Some links are set up to only count bus numberplates for public transport journey times\n",
    "            if systemCodeNumber.endswith('_BUS') or \\\n",
    "               'latest' not in feed['timeseries'][0]:\n",
    "                continue\n",
    "            \n",
    "            if not systemCodeNumber in anprLinks:\n",
    "                linkDescription = journeyTimeLink['meta'].copy()\n",
    "                linkDescription['timeseriesIRIs'] = {}\n",
    "                anprLinks[systemCodeNumber] = linkDescription\n",
    "                print('Discovered monitoring link \"%s\"' % anprLinks[systemCodeNumber]['longName'].strip())\n",
    "            \n",
    "            anprLinks[systemCodeNumber]['systemCodeNumber'] = systemCodeNumber\n",
    "            \n",
    "            for ts in feed['timeseries']:\n",
    "                timeseriesType = None\n",
    "                \n",
    "                if feed['metric'] == 'Journey time':\n",
    "                    timeseriesType = 'timeseriesJourneyTime'\n",
    "                elif feed['metric'] == 'Number plates at start of link':\n",
    "                    timeseriesType = 'timeseriesPlatesIn'\n",
    "                elif feed['metric'] == 'Number plates at end of link':\n",
    "                    timeseriesType = 'timeseriesPlatesOut'\n",
    "                    \n",
    "                for link in ts['links']:\n",
    "                    if link['rel'] == 'archives' and timeseriesType is not None:\n",
    "                        anprLinks[systemCodeNumber]['timeseriesIRIs'][timeseriesType] = link['href']\n",
    "            \n",
    "            nameElements = anprNameMatcher.match(anprLinks[systemCodeNumber]['longName'])\n",
    "\n",
    "            if nameElements is None:\n",
    "                print('Unable to match name \"%s\". Skipping.' % anprLinks[systemCodeNumber]['longName'])\n",
    "                del anprLinks[systemCodeNumber]\n",
    "                continue\n",
    "\n",
    "            anprLinks[systemCodeNumber]['highwayDescription'] = nameElements[1]\n",
    "            anprLinks[systemCodeNumber]['startDescription'] = nameElements[2]\n",
    "            anprLinks[systemCodeNumber]['endDescription'] = nameElements[3]\n",
    "\n",
    "anprLinks = pd.DataFrame.from_records(list(anprLinks.values()), index=['systemCodeNumber'])\n",
    "print('Discovered %u ANPR pairs.' % len(anprLinks.index))\n",
    "\n",
    "anprLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert links into a list of measurement points\n",
    "\n",
    "anprPoints = {}\n",
    "anprDirectionMatcher = re.compile('^.*\\((.*)\\)$')\n",
    "anprDuplicateRegister = []\n",
    "\n",
    "for systemCodeNumber in anprLinks.index:\n",
    "    linkDefinition = anprLinks[anprLinks.index == systemCodeNumber]\n",
    "    for end in ['start', 'end']:\n",
    "        coordinates = (linkDefinition[end + 'Easting'].values[0], linkDefinition[end + 'Northing'].values[0])\n",
    "        pointDescription = linkDefinition[end + 'Description'].values[0].strip()\n",
    "        highwayDescription = linkDefinition['highwayDescription'].values[0].strip()\n",
    "        countDirection = anprDirectionMatcher.match(linkDefinition['highwayDescription'].values[0].strip())\n",
    "        vectorId = '%s (%s)' % (coordinates, countDirection)\n",
    "        \n",
    "        if countDirection is None:\n",
    "            print('Unable to find direction in description \"%s\"' % highwayDescription)\n",
    "            countDirection = 'Unknown'\n",
    "        else:\n",
    "            countDirection = countDirection[1]\n",
    "        \n",
    "        #print(coordinates, pointDescription, highwayDescription)\n",
    "        \n",
    "        if vectorId in anprPoints:\n",
    "            anprPoints[vectorId]['linkCount'] = anprPoints[vectorId]['linkCount'] + 1\n",
    "            anprDuplicateRegister.append({\n",
    "                'originalId': systemCodeNumber,\n",
    "                'originalEnd': end,\n",
    "                'isAlias': True,\n",
    "                'timeseriesName': anprPoints[vectorId]['timeseriesName']\n",
    "            })\n",
    "        else:\n",
    "            vehicleCountName = '%s.%s.%s' % (systemCodeNumber, end, countDirection.lower())\n",
    "            anprPoints[vectorId] = {\n",
    "                'systemCodeNumber': systemCodeNumber,\n",
    "                'end': end,\n",
    "                'timeseriesName': vehicleCountName,\n",
    "                'pointDescription': pointDescription,\n",
    "                'highwayDescription': highwayDescription,\n",
    "                'easting': linkDefinition[end + 'Easting'].values[0],\n",
    "                'northing': linkDefinition[end + 'Northing'].values[0],\n",
    "                'linkCount': 1\n",
    "            }\n",
    "            anprDuplicateRegister.append({\n",
    "                'alternativeId': systemCodeNumber,\n",
    "                'originalEnd': end,\n",
    "                'isAlias': False,\n",
    "                'timeseriesName': anprPoints[vectorId]['timeseriesName']\n",
    "            })\n",
    "            \n",
    "anprPoints = pd.DataFrame.from_records(list(anprPoints.values()))\n",
    "\n",
    "print('Found %u unique monitoring points.' % len(anprPoints.index))\n",
    "anprPoints.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdAnprDuplicateRegister = pd.DataFrame.from_records(anprDuplicateRegister)\n",
    "pdAnprDuplicateRegister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLinkHistoric = pd.read_pickle('../cache/baseline-traffic-volumes-pd.pkl')\n",
    "dfLinkHistoric.index = dfLinkHistoric.index.tz_localize(tzUTC).tz_convert(tzLocal)\n",
    "dfLinkHistoric.sort_index(inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "dfLinkHistoric.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daysPerRequest = 30\n",
    "\n",
    "dfPointTs = None\n",
    "\n",
    "for pointIndex in anprPoints.index:\n",
    "    pointRow = anprPoints[anprPoints.index == pointIndex]\n",
    "    point = pointRow.to_dict(orient='records')[0]\n",
    "    systemCodeNumber = point['systemCodeNumber']\n",
    "    linkRow = anprLinks[anprLinks.index == systemCodeNumber]\n",
    "    link = linkRow.to_dict(orient='records')[0]\n",
    "    linkIRIs = link['timeseriesIRIs']\n",
    "    \n",
    "    #vehicleCountName = '%s at %s' % (point['highwayDescription'], point['pointDescription'])\n",
    "    vehicleCountName = point['timeseriesName'] # '%s.%s' % (point['systemCodeNumber'], point['end'])\n",
    "    vehicleCountIRIRequired = 'timeseriesPlatesIn' if point['end'] == 'start' else 'timeseriesPlatesOut'\n",
    "    \n",
    "    historicColumn = '%s.%s' % (\n",
    "        systemCodeNumber,\n",
    "        'platesIn' if point['end'] == 'start' else 'platesOut'\n",
    "    )\n",
    "    \n",
    "    if vehicleCountIRIRequired not in linkIRIs:\n",
    "        print('No data available for %s' % vehicleCountName)\n",
    "        continue\n",
    "    \n",
    "    vehicleCountIRI = linkIRIs[vehicleCountIRIRequired]\n",
    "    \n",
    "    print(vehicleCountName)\n",
    "    print('  [', end='')\n",
    "    \n",
    "    # TODO: Load the base data here instead\n",
    "    pointTimeseries = None\n",
    "    \n",
    "    for windowStart in dateutil.rrule.rrule(\n",
    "        dateutil.rrule.DAILY,\n",
    "        interval=daysPerRequest,\n",
    "        dtstart=previousDataEnd + pd.Timedelta(seconds=1),\n",
    "        until=dateToday + pd.Timedelta(hours=24)\n",
    "    ):\n",
    "        windowEnd = windowStart + pd.Timedelta(days=daysPerRequest) - pd.Timedelta(seconds=1)\n",
    "\n",
    "        if windowEnd > dateToday + pd.Timedelta(hours=24):\n",
    "            windowEnd = dateToday + pd.Timedelta(hours=24)\n",
    "            \n",
    "        windowResponse = json.loads(\n",
    "            urllib.request.urlopen(\n",
    "              '%s?startTime=%s&endTime=%s' % (vehicleCountIRI, windowStart.isoformat().replace('+00:00', 'Z'), windowEnd.isoformat().replace('+00:00', 'Z'))\n",
    "            ).read().decode('utf-8')\n",
    "        )['historic']['values']\n",
    "\n",
    "        if pointTimeseries is None:\n",
    "            pointTimeseries = windowResponse\n",
    "        else:\n",
    "            pointTimeseries.extend(windowResponse)\n",
    "        print('.', end='')\n",
    "        \n",
    "    print(']')\n",
    "    \n",
    "    if np.sum(list(map(lambda v: v['value'], pointTimeseries))) < 1:\n",
    "        print('Empty timeseries returned for %s' % vehicleCountName)\n",
    "        continue\n",
    "    \n",
    "    #if pointTimeseries is not None:\n",
    "    #    break\n",
    "\n",
    "    dfPoint = pd.DataFrame \\\n",
    "        .from_records(pointTimeseries, exclude=['duration']) \\\n",
    "        .rename(columns={'value': vehicleCountName})\n",
    "    dfPoint['time'] = dfPoint['time'].apply(lambda t: datetime.datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%fZ\").replace(tzinfo=tzUTC).astimezone(tzLocal))\n",
    "    dfPoint.set_index('time', inplace=True, drop=True)\n",
    "    \n",
    "    if dfPointInterpTsOld is not None and vehicleCountName in dfPointInterpTsOld.columns:\n",
    "        dfPoint = pd.concat([\n",
    "            dfPointInterpTsOld[vehicleCountName],\n",
    "            dfPoint[vehicleCountName]\n",
    "        ]).to_frame()\n",
    "    elif historicColumn in dfLinkHistoric.columns:\n",
    "        dfPoint = pd.concat([\n",
    "            dfLinkHistoric.rename(columns={ historicColumn: vehicleCountName })[vehicleCountName],\n",
    "            dfPoint[vehicleCountName]\n",
    "        ]).to_frame()\n",
    "    \n",
    "    # Underlying data is 4 minute blocks, make it 8 minutes\n",
    "    pointDataStart = np.min(dfPoint.index)\n",
    "    dfPoint = dfPoint.resample('480s', kind='timestamp', base=0).sum(min_count=1)\n",
    "    gc.collect()\n",
    "    \n",
    "    if dfPointTs is None:\n",
    "        dfPointTs = dfPoint\n",
    "    else:\n",
    "        dfPointTs = dfPointTs.join(\n",
    "            dfPoint, \n",
    "            how='outer',\n",
    "            rsuffix=' (%s)' % systemCodeNumber\n",
    "        )\n",
    "        dfPoint = None\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "dfPointTs.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPointTs.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate the index\n",
    "dfPointTs = dfPointTs.loc[~dfPointTs.index.duplicated(keep='last')]\n",
    "dfPointTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPointTs.to_pickle('../cache/recent-traffic-volumes-pd.pkl')\n",
    "anprLinks.to_pickle('../cache/recent-traffic-volumes-link-metadata-pd.pkl')\n",
    "anprPoints.to_pickle('../cache/recent-traffic-volumes-point-metadata-pd.pkl')\n",
    "pdAnprDuplicateRegister.to_pickle('../cache/recent-traffic-volumes-point-alternative-ids.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorry folks, but it's an >80MB file otherwise\n",
    "dfPointTs.resample('960s').sum().to_csv('../output/t&w-anpr-volumes-pd-16min.csv')\n",
    "anprLinks.to_csv('../output/t&w-anpr-volumes-link-metadata-pd.csv')\n",
    "anprPoints.to_csv('../output/t&w-anpr-volumes-point-metadata-pd.csv')\n",
    "pdAnprDuplicateRegister.to_csv('../output/t&w-anpr-volumes-point-metadata-original-id-mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
